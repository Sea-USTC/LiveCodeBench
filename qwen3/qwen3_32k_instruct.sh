VLLM_ATTENTION_BACKEND=FLASHINFER python -m lcb_runner.runner.main --model Qwen/Qwen3-4B-Instruct-2507 --scenario codegeneration --evaluate --release_version v6 --max_tokens 32768 --tensor_parallel_size 4 --use_cache --kv_cache_quantized --temperature 0.6